{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"self_supervised_jigsaw.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"TtdYJd8pl4OH","colab_type":"code","outputId":"7925c147-f552-4251-b0c4-4adc7b34b97b","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1561021252679,"user_tz":-60,"elapsed":362795,"user":{"displayName":"Aras Yıldız","photoUrl":"","userId":"09645095620751874495"}}},"source":["%reload_ext autoreload\n","%autoreload 2\n","%matplotlib inline\n","\n","#DOWNLOAD THE DATA\n","from google.colab import drive\n","import zipfile\n","\n","#MOUNT GDRIVE: DOWNLOAD DATA AND FILTERED LABELS\n","drive.mount('/content/gdrive',force_remount=True)\n","\n","# UNZIP ZIP\n","print (\"Uncompressing zip file\")\n","zip_ref = zipfile.ZipFile('/content/gdrive/My Drive/CheXpert-v1.0-small.zip', 'r')\n","zip_ref.extractall()\n","zip_ref.close()\n","print(\"downloaded files\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n","Uncompressing zip file\n","downloaded files\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NA_ZEVnZl_v9","colab_type":"code","colab":{}},"source":["#import sys #For Patch lib\n","#sys.path.append(\"gdrive/My Drive/patch\")\n","\n","#CONTRASTIVE LOST OLCAK"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qgi8EgUGmIPM","colab_type":"code","colab":{}},"source":["import os#for CUDA tracking\n","#from __future__ import print_function, division\n","import torch\n","import pandas as pd\n","from skimage.io import imread\n","#from skimage import io\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","#MODELS\n","import re\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","import matplotlib.pyplot as plt\n","\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","class CheXpertDataset(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","              \n","        super().__init__()\n","\n","        self.observations_frame = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.list_classes=['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n","\n","    def __len__(self):\n","        return len(self.observations_frame)\n","\n","    def __getitem__(self, idx):\n","        \n","        img_name = os.path.join(self.observations_frame.iloc[idx, 0])\n","        image = imread(img_name)\n","        image = transforms.ToPILImage()(image)\n","\n","        observations=self.observations_frame.loc[idx,self.list_classes]\n","        observations = torch.from_numpy(observations.values.astype(np.float32))\n","        \n","        #RETURNING IMAGE AND LABEL SEPERATELY FOR TORCH TRANSFORM LIB\n","      \n","        if self.transform:     \n","            image = self.transform(image)\n","                \n","        return image,observations\n","      \n","      \n","      \n","def chexpert_load(csv_file_name,transformation,batch_size):\n","  \n","  cheXpert_dataset = CheXpertDataset(csv_file=csv_file_name,\n","                                     root_dir='not used', transform=transformation)\n","\n","  dataloader = DataLoader(cheXpert_dataset, batch_size=batch_size,shuffle=True)\n","\n","  return cheXpert_dataset,dataloader\n","\n","\n","\n","use_cuda = True\n","if use_cuda and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    print(\"CUDA didn't work\")\n","    device = torch.device('cpu')\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJKI79wP6INs","colab_type":"code","outputId":"fdb079aa-0d2e-4874-9e9b-8f449e072f2c","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["for i in range(0,3):\n","  print(i)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","1\n","2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QVMh9LJCmc2d","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches4rectangle\n","\n","\n","class Jigsaw(object):\n","    def __init__(self, image_batch, path_permutation_set, grid_crop_size=255, patch_crop_size=75, show=False):\n","\n","        self.bs, _, self.h, self.w = image_batch.shape\n","        self.show = show  # print cropped images and show where they are 1 time for each batch\n","        self.image_batch = image_batch\n","        self.permutation_set = torch.load(path_permutation_set)\n","        self.grid_crop_size = grid_crop_size\n","        self.patch_crop_size = patch_crop_size\n","\n","    def __call__(self):\n","\n","        patches = torch.Tensor()\n","        labels = np.empty(self.bs)\n","        \n","        for idx, image in enumerate(self.image_batch):\n","\n","            permuted_patches, label ,shiftcol,shiftrow, cord_patches = self.puzzled_patches(image)\n","\n","            if idx == 1 and self.show:\n","                self.show_cropped_patches(image, shiftcol, shiftrow, permuted_patches, cord_patches, label)\n","                self.show = False\n","\n","            patches = torch.cat((patches, permuted_patches))\n","            labels[idx] = label\n","        \n","            \n","        return patches, labels\n","\n","      \n","    def puzzled_patches(self, image):\n","      \n","        grid, shiftcol, shiftrow = self.random_crop(image,self.grid_crop_size)\n","        patches, cord_patches = self.get_patches(grid)\n","        permuted_patches,label= self.permut(patches)\n","        \n","        return permuted_patches, label ,shiftcol,shiftrow, cord_patches\n","\n","    def random_crop(self, image,cropsize):\n","      _, h ,_ = image.shape\n","      [shiftrow, shiftcol] = np.random.randint(h-cropsize-1, size=2)\n","      \n","      return image[:,shiftrow:shiftrow+cropsize,shiftcol:shiftcol+cropsize] , shiftcol, shiftrow\n","\n","    def get_patches(self,grid):\n","      \n","      #dive a 225x225 grid to 3x3 \n","      patches = torch.empty(9,self.patch_crop_size, self.patch_crop_size)\n","      cord_patches= []\n","      grid_patch_size = int(self.grid_crop_size/3) #85   \n","     \n","      patch_n=0\n","      for i in range(0,3):\n","        for j in range(0,3):\n","          #each patch take random crop to prevent edge following\n","          \n","          row_start=0+grid_patch_size*i ; row_finit=row_start+grid_patch_size\n","          col_start=0+grid_patch_size*j ; col_finit=col_start+grid_patch_size\n","          \n","          patches[patch_n,:,:] , scol,srow =self.random_crop(grid[:,row_start:row_finit,col_start:col_finit],self.patch_crop_size)\n","          cord_patches.append((scol,srow))\n","          patch_n=patch_n+1\n","      \n","      return patches , cord_patches\n","      \n","    def permut(self,patches):\n","      set_size,_= self.permutation_set.shape# Nx9\n","      \n","      pick = np.random.randint(set_size)# pick = label of the perm\n","      perm = self.permutation_set[pick]\n","      \n","      permuted_patches=patches[perm.long()]\n","      \n","      return permuted_patches, pick\n","           \n","    \n","    def show_cropped_patches(self, image, shift_col, shift_row, permuted_patches, cord_patches, label):\n","      #Preparing        \n","      image_draw = transforms.ToPILImage()(image)\n","      perm_set= self.permutation_set[label]\n","\n","      #font = ImageFont.truetype(\"arial.ttf\", fontsize)\n","      #Original Image plot\n","      fig, ax = plt.subplots(1,figsize=(5,5))\n","      ax.axis(\"off\")\n","      \n","      start_col = shift_col\n","      start_row = shift_row\n","      \n","      for i in range(0,4):\n","        grid_patch_size = int(self.grid_crop_size/3)\n","        shift= i*grid_patch_size\n","        \n","        ax.plot([start_col, start_col+grid_patch_size*3], [start_row+shift,start_row+shift],color='r',linestyle='dashed')\n","        ax.plot([start_col+shift,start_col+shift], [start_row, start_row+grid_patch_size*3],color='r',linestyle='dashed')\n","\n","      #print(cord_patches)\n","      for idx, (srow, scol) in enumerate(cord_patches):\n","        #print(idx)\n","        n= perm_set[idx]\n","        \n","        if n<3:\n","           i = 0\n","        elif n < 6:\n","          i = 1\n","        else:\n","          i=2\n","          \n","        j = n %3\n","        \n","        rect = patches4rectangle.Rectangle((shift_col +j*grid_patch_size+scol, shift_row + i*grid_patch_size +srow), 64 , 64, linewidth=2,\n","                                           edgecolor='g', facecolor='none')\n","        ax.add_patch(rect)\n","\n","          \n","      plt.imshow(image_draw, cmap='Greys_r')\n","        \n","      #Permuted Grid\n","      fig, ax = plt.subplots(3,3, sharex='col', sharey='row',figsize=(5,5))\n","      fig.subplots_adjust(hspace=0,wspace=0)\n","        \n","      for idx in np.arange(9):\n","          \n","        if idx<3:\n","          i = 0\n","        elif idx < 6:\n","          i = 1\n","        else:\n","          i=2\n","          \n","        j = idx %3\n","\n","        patch_show = transforms.ToPILImage()(permuted_patches[idx,:,:])\n","        ax[i,j].imshow(patch_show, cmap='Greys_r')\n","        ax[i,j].axis('off')\n","       \n","      #Aligned Grid\n","      fig, ax = plt.subplots(3,3, sharex='col', sharey='row',figsize=(5,5))\n","      fig.subplots_adjust(hspace=0,wspace=0)\n","      \n","      for index,idx in enumerate(perm_set):\n","        \n","        idx = int(idx.item())\n","        \n","        if idx<3:\n","          i = 0\n","        elif idx < 6:\n","          i = 1\n","        else:\n","          i=2\n","          \n","        j = idx %3\n","          \n","        patch_show = transforms.ToPILImage()(permuted_patches[index,:,:])\n","        ax[i,j].imshow(patch_show, cmap='Greys_r')\n","        ax[i,j].axis('off')\n","            \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RET3-mB5jqGW","colab_type":"code","colab":{}},"source":["#HOW TO USE RES BLOCK AT THE END AS A Class there operations between features and classifier that causes dimension conflict\n","#LOSS FUNC JUST A 8 WAY SOFTMAX\n","\n","class JigSawHead(torch.nn.Module):\n","    def __init__(self, D_in, D_out=8):\n","        \"\"\"\n","        In the constructor we instantiate two nn.Linear modules and assign them as\n","        member variables.\n","        \"\"\"\n","        super(RelativePositionHead, self).__init__()\n","        self.res_block1 = models.resnet.BasicBlock(D_in,512)\n","        self.res_block2 = models.resnet.BasicBlock(512,512)\n","        self.classifier = torch.nn.Linear(512,D_out)\n","\n","    def forward(self, x):\n","        \n","        #print(x.shape)\n","        x = self.res_block1(x)\n","        x = self.res_block2(x)\n","        \n","        _,N,_,_ = x.shape\n","        # combining two representation (batch is [bs,ch,h,w ])\n","        x = torch.stack([torch.cat((x[idx,:,:,:],x[idx+1,:,:,:]))\n","                     for idx in range(0,N,2)], dim=0)\n","        \n","        #linear output with 8 outpts(directions)\n","        y_pred = self.classifier(x)\n","        return y_pred\n","\n","      \n","      \n","class Basic_JigsawHead(torch.nn.Module):\n","    def __init__(self, D_in, D_out):\n","        \"\"\"\n","        No task head just concating what comes out just before default classifer then applying linear\n","        In the constructor we instantiate two nn.Linear modules and assign them as\n","        member variables.\n","        \"\"\"\n","        super(Basic_JigsawHead, self).__init__()\n","        self.classifier = torch.nn.Linear(D_in*9,D_out)\n","\n","    def forward(self, x):        \n","        N,_ = x.shape\n","        # combining two representation (batch is [bs,ch,h,w ])\n","        \n","        def stack_tiles(tiles):\n","          tile_stacked = torch.Tensor().cuda()\n","          for tile in tiles:\n","            tile_stacked = torch.cat((tile_stacked,tile))\n","          return tile_stacked\n","          \n","        x = torch.stack([stack_tiles(x[idx:idx+9,:]) for idx in range(0,N,9)], dim=0).cuda()\n","        \n","        #linear output with 8 outpts(directions)\n","        y_pred = self.classifier(x)\n","        return y_pred\n","    \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-sRIwg7mc7D","colab_type":"code","outputId":"12d82647-203c-4bcc-88e8-3aa3c34838cd","colab":{"base_uri":"https://localhost:8080/","height":2754}},"source":["learning_rate=0.0001\n","batch_size=16\n","dtype = torch.float32\n","resize=320\n","num_classes=50\n","grid_crop_size=225\n","patch_crop_size=64\n","\n","file_name_p_set = F\"permutation_set_{num_classes}.pt\"\n","PATH_p_set = F\"gdrive/My Drive/summerthesis/permutation_sets/{file_name_p_set}\"\n","\n","#just ToTensor before pathch\n","transform_train= transforms.Compose([          transforms.RandomCrop(320),\n","                                               transforms.ToTensor()])\n","\n","#after patch transformation\n","transform_after_jigsaw_patch= transforms.Compose([    transforms.ToPILImage(),\n","                                               #transforms.Resize(resize),\n","                                               transforms.ToTensor(),\n","                                               transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),\n","                                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])                                 \n","\n","cheXpert_train_dataset, dataloader = chexpert_load(\"CheXpert-v1.0-small/train.csv\",transform_train,batch_size)\n","\n","\n","\n","model=models.densenet121(num_classes=num_classes)\n","#model.classifier=RelativePositionHead(1024,num_classes)\n","model.classifier=Basic_JigsawHead(1024,num_classes)\n","model=model.to(device=device)\n","\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","criterion =torch.nn.CrossEntropyLoss().cuda()\n","\n","num_epochs=5\n","for epoch in range(num_epochs):\n","    for i,  (images, observations) in enumerate(dataloader):   # Load a batch of images with its (index, data, class)\n","        \n","        \n","        jigsaw_patcher = Jigsaw(images, PATH_p_set, grid_crop_size=grid_crop_size, patch_crop_size=patch_crop_size, show=False)\n","        jigsaw_patches, labels =  jigsaw_patcher()\n","        jigsaw_patches = torch.stack([transform_after_jigsaw_patch(x_i) \n","                     for i, x_i in enumerate(torch.unbind(jigsaw_patches, dim=0))], dim=0)\n","                \n","          \n","        jigsaw_patches = jigsaw_patches.to(device=device, dtype=dtype)\n","        labels = torch.from_numpy(labels)\n","        labels = labels.long()\n","        labels = labels.to(device=device, dtype=torch.long)\n","        \n","        \n","        outputs = model(jigsaw_patches)                             # Forward pass: compute the output class given a image\n","        \n","        #print(\"model outputs \",outputs.shape)\n","\n","        loss = criterion(outputs, labels)           # Compute the loss: difference between the output class and the pre-given label\n","\n","        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n","        loss.backward()                                   # Backward pass: compute the weight\n","        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n","        \n","        \n","        if (i+1) % 50 == 0:                              # Logging\n","            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n","                 %(epoch+1, num_epochs, i+1, len(cheXpert_train_dataset)//batch_size, loss))\n","\n","\n","print('training done')\n","\n","\n","\n","file_name = F\"jigsaw_num_classes{num_classes}_epoch{num_epochs}_batch{batch_size}_grid_size{grid_crop_size}_patch_size{patch_crop_size}.pth\"\n","PATH=F\"/content/gdrive/My Drive/summerthesis/saved_model/{file_name}\"\n","torch.save(model.state_dict(), PATH)\n","print('saved  model to google drive')\n","\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch [1/5], Step [50/13963], Loss: 3.1644\n","Epoch [1/5], Step [100/13963], Loss: 3.1263\n","Epoch [1/5], Step [150/13963], Loss: 2.4570\n","Epoch [1/5], Step [200/13963], Loss: 1.8843\n","Epoch [1/5], Step [250/13963], Loss: 1.3998\n","Epoch [1/5], Step [300/13963], Loss: 1.3370\n","Epoch [1/5], Step [350/13963], Loss: 0.8828\n","Epoch [1/5], Step [400/13963], Loss: 0.6529\n","Epoch [1/5], Step [450/13963], Loss: 0.9817\n","Epoch [1/5], Step [500/13963], Loss: 0.4822\n","Epoch [1/5], Step [550/13963], Loss: 0.5752\n","Epoch [1/5], Step [600/13963], Loss: 0.3148\n","Epoch [1/5], Step [650/13963], Loss: 0.2749\n","Epoch [1/5], Step [700/13963], Loss: 0.4265\n","Epoch [1/5], Step [750/13963], Loss: 0.2435\n","Epoch [1/5], Step [800/13963], Loss: 0.1025\n","Epoch [1/5], Step [850/13963], Loss: 0.1357\n","Epoch [1/5], Step [900/13963], Loss: 0.0935\n","Epoch [1/5], Step [950/13963], Loss: 0.1925\n","Epoch [1/5], Step [1000/13963], Loss: 0.3215\n","Epoch [1/5], Step [1050/13963], Loss: 0.2519\n","Epoch [1/5], Step [1100/13963], Loss: 0.3510\n","Epoch [1/5], Step [1150/13963], Loss: 0.4803\n","Epoch [1/5], Step [1200/13963], Loss: 0.2553\n","Epoch [1/5], Step [1250/13963], Loss: 0.1088\n","Epoch [1/5], Step [1300/13963], Loss: 0.2089\n","Epoch [1/5], Step [1350/13963], Loss: 0.1813\n","Epoch [1/5], Step [1400/13963], Loss: 1.2367\n","Epoch [1/5], Step [1450/13963], Loss: 0.1981\n","Epoch [1/5], Step [1500/13963], Loss: 0.3985\n","Epoch [1/5], Step [1550/13963], Loss: 0.0842\n","Epoch [1/5], Step [1600/13963], Loss: 0.0969\n","Epoch [1/5], Step [1650/13963], Loss: 0.1327\n","Epoch [1/5], Step [1700/13963], Loss: 0.1917\n","Epoch [1/5], Step [1750/13963], Loss: 0.2685\n","Epoch [1/5], Step [1800/13963], Loss: 0.2589\n","Epoch [1/5], Step [1850/13963], Loss: 0.0725\n","Epoch [1/5], Step [1900/13963], Loss: 0.8497\n","Epoch [1/5], Step [1950/13963], Loss: 0.2315\n","Epoch [1/5], Step [2000/13963], Loss: 0.0568\n","Epoch [1/5], Step [2050/13963], Loss: 0.3908\n","Epoch [1/5], Step [2100/13963], Loss: 0.1438\n","Epoch [1/5], Step [2150/13963], Loss: 0.0477\n","Epoch [1/5], Step [2200/13963], Loss: 0.4404\n","Epoch [1/5], Step [2250/13963], Loss: 0.4344\n","Epoch [1/5], Step [2300/13963], Loss: 0.5257\n","Epoch [1/5], Step [2350/13963], Loss: 0.1260\n","Epoch [1/5], Step [2400/13963], Loss: 0.5855\n","Epoch [1/5], Step [2450/13963], Loss: 0.4778\n","Epoch [1/5], Step [2500/13963], Loss: 0.1883\n","Epoch [1/5], Step [2550/13963], Loss: 0.1755\n","Epoch [1/5], Step [2600/13963], Loss: 0.0963\n","Epoch [1/5], Step [2650/13963], Loss: 0.1336\n","Epoch [1/5], Step [2700/13963], Loss: 0.0404\n","Epoch [1/5], Step [2750/13963], Loss: 0.4502\n","Epoch [1/5], Step [2800/13963], Loss: 0.0718\n","Epoch [1/5], Step [2850/13963], Loss: 0.1231\n","Epoch [1/5], Step [2900/13963], Loss: 0.0998\n","Epoch [1/5], Step [2950/13963], Loss: 0.5417\n","Epoch [1/5], Step [3000/13963], Loss: 0.0459\n","Epoch [1/5], Step [3050/13963], Loss: 0.0537\n","Epoch [1/5], Step [3100/13963], Loss: 0.2495\n","Epoch [1/5], Step [3150/13963], Loss: 0.4107\n","Epoch [1/5], Step [3200/13963], Loss: 0.0437\n","Epoch [1/5], Step [3250/13963], Loss: 0.0467\n","Epoch [1/5], Step [3300/13963], Loss: 0.0488\n","Epoch [1/5], Step [3350/13963], Loss: 0.0465\n","Epoch [1/5], Step [3400/13963], Loss: 0.3386\n","Epoch [1/5], Step [3450/13963], Loss: 0.1057\n","Epoch [1/5], Step [3500/13963], Loss: 0.2617\n","Epoch [1/5], Step [3550/13963], Loss: 0.5933\n","Epoch [1/5], Step [3600/13963], Loss: 0.1898\n","Epoch [1/5], Step [3650/13963], Loss: 0.1415\n","Epoch [1/5], Step [3700/13963], Loss: 0.0790\n","Epoch [1/5], Step [3750/13963], Loss: 0.5901\n","Epoch [1/5], Step [3800/13963], Loss: 0.0858\n","Epoch [1/5], Step [3850/13963], Loss: 0.1862\n","Epoch [1/5], Step [3900/13963], Loss: 0.0674\n","Epoch [1/5], Step [3950/13963], Loss: 0.2715\n","Epoch [1/5], Step [4000/13963], Loss: 0.1126\n","Epoch [1/5], Step [4050/13963], Loss: 0.0518\n","Epoch [1/5], Step [4100/13963], Loss: 2.0130\n","Epoch [1/5], Step [4150/13963], Loss: 0.0123\n","Epoch [1/5], Step [4200/13963], Loss: 0.1361\n","Epoch [1/5], Step [4250/13963], Loss: 0.1065\n","Epoch [1/5], Step [4300/13963], Loss: 0.1698\n","Epoch [1/5], Step [4350/13963], Loss: 0.0531\n","Epoch [1/5], Step [4400/13963], Loss: 0.0354\n","Epoch [1/5], Step [4450/13963], Loss: 0.0599\n","Epoch [1/5], Step [4500/13963], Loss: 0.0355\n","Epoch [1/5], Step [4550/13963], Loss: 0.0153\n","Epoch [1/5], Step [4600/13963], Loss: 0.0559\n","Epoch [1/5], Step [4650/13963], Loss: 0.4193\n","Epoch [1/5], Step [4700/13963], Loss: 0.1007\n","Epoch [1/5], Step [4750/13963], Loss: 0.0183\n","Epoch [1/5], Step [4800/13963], Loss: 0.3240\n","Epoch [1/5], Step [4850/13963], Loss: 0.0383\n","Epoch [1/5], Step [4900/13963], Loss: 0.0314\n","Epoch [1/5], Step [4950/13963], Loss: 0.4976\n","Epoch [1/5], Step [5000/13963], Loss: 0.1844\n","Epoch [1/5], Step [5050/13963], Loss: 0.0144\n","Epoch [1/5], Step [5100/13963], Loss: 0.8800\n","Epoch [1/5], Step [5150/13963], Loss: 0.0256\n","Epoch [1/5], Step [5200/13963], Loss: 0.1741\n","Epoch [1/5], Step [5250/13963], Loss: 0.0275\n","Epoch [1/5], Step [5300/13963], Loss: 0.1850\n","Epoch [1/5], Step [5350/13963], Loss: 0.0250\n","Epoch [1/5], Step [5400/13963], Loss: 0.0333\n","Epoch [1/5], Step [5450/13963], Loss: 0.2418\n","Epoch [1/5], Step [5500/13963], Loss: 0.0091\n","Epoch [1/5], Step [5550/13963], Loss: 0.3908\n","Epoch [1/5], Step [5600/13963], Loss: 0.0252\n","Epoch [1/5], Step [5650/13963], Loss: 0.0156\n","Epoch [1/5], Step [5700/13963], Loss: 0.2824\n","Epoch [1/5], Step [5750/13963], Loss: 0.0377\n","Epoch [1/5], Step [5800/13963], Loss: 0.1645\n","Epoch [1/5], Step [5850/13963], Loss: 0.0628\n","Epoch [1/5], Step [5900/13963], Loss: 0.0340\n","Epoch [1/5], Step [5950/13963], Loss: 0.1000\n","Epoch [1/5], Step [6000/13963], Loss: 0.0642\n","Epoch [1/5], Step [6050/13963], Loss: 0.4900\n","Epoch [1/5], Step [6100/13963], Loss: 0.5814\n","Epoch [1/5], Step [6150/13963], Loss: 0.0123\n","Epoch [1/5], Step [6200/13963], Loss: 0.0113\n","Epoch [1/5], Step [6250/13963], Loss: 0.3255\n","Epoch [1/5], Step [6300/13963], Loss: 0.4749\n","Epoch [1/5], Step [6350/13963], Loss: 0.1079\n","Epoch [1/5], Step [6400/13963], Loss: 0.0473\n","Epoch [1/5], Step [6450/13963], Loss: 0.1446\n","Epoch [1/5], Step [6500/13963], Loss: 0.0202\n","Epoch [1/5], Step [6550/13963], Loss: 0.8865\n","Epoch [1/5], Step [6600/13963], Loss: 0.2889\n","Epoch [1/5], Step [6650/13963], Loss: 0.0265\n","Epoch [1/5], Step [6700/13963], Loss: 0.6524\n","Epoch [1/5], Step [6750/13963], Loss: 0.0011\n","Epoch [1/5], Step [6800/13963], Loss: 0.2061\n","Epoch [1/5], Step [6850/13963], Loss: 0.0197\n","Epoch [1/5], Step [6900/13963], Loss: 0.0173\n","Epoch [1/5], Step [6950/13963], Loss: 0.2495\n","Epoch [1/5], Step [7000/13963], Loss: 0.0033\n","Epoch [1/5], Step [7050/13963], Loss: 0.2008\n","Epoch [1/5], Step [7100/13963], Loss: 0.4022\n","Epoch [1/5], Step [7150/13963], Loss: 0.0158\n","Epoch [1/5], Step [7200/13963], Loss: 0.5181\n","Epoch [1/5], Step [7250/13963], Loss: 0.1081\n","Epoch [1/5], Step [7300/13963], Loss: 0.2030\n","Epoch [1/5], Step [7350/13963], Loss: 0.0198\n","Epoch [1/5], Step [7400/13963], Loss: 0.1303\n","Epoch [1/5], Step [7450/13963], Loss: 0.2454\n","Epoch [1/5], Step [7500/13963], Loss: 0.0244\n","Epoch [1/5], Step [7550/13963], Loss: 0.1038\n","Epoch [1/5], Step [7600/13963], Loss: 0.0557\n","Epoch [1/5], Step [7650/13963], Loss: 0.0157\n","Epoch [1/5], Step [7700/13963], Loss: 0.0191\n","Epoch [1/5], Step [7750/13963], Loss: 0.5880\n","Epoch [1/5], Step [7800/13963], Loss: 0.2560\n","Epoch [1/5], Step [7850/13963], Loss: 0.0065\n","Epoch [1/5], Step [7900/13963], Loss: 0.0541\n","Epoch [1/5], Step [7950/13963], Loss: 0.0538\n","Epoch [1/5], Step [8000/13963], Loss: 0.2540\n","Epoch [1/5], Step [8050/13963], Loss: 0.0079\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3gM002QjNj9S","colab_type":"code","colab":{}},"source":["\n","\n"],"execution_count":0,"outputs":[]}]}